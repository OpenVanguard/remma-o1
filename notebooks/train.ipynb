{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing the imports\n",
    "\n",
    "In this section, we import the necessary libraries and modules required for our workflow. This includes essential packages for data handling, model building, training utilities, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_from_disk\n",
    "from src.modeling import build_model\n",
    "from src.training.utils.checkpoint import save_checkpoint\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiDomainDataset(Dataset):\n",
    "    def __init__(self, tokenized_data, block_size=2048):\n",
    "        self.data = tokenized_data\n",
    "        self.block_size = block_size\n",
    "        self.domain_weights = {\n",
    "            'c4': 0.7,\n",
    "            'wikipedia': 0.2,\n",
    "            'math': 0.1\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        # Ensure proper sequence length handling\n",
    "        chunk = example[\"input_ids\"][:self.block_size].tolist()\n",
    "        # Add padding if needed\n",
    "        if len(chunk) < self.block_size:\n",
    "            chunk += [0] * (self.block_size - len(chunk))\n",
    "        \n",
    "        # Domain-aware processing\n",
    "        domain = example.get(\"domain\", \"c4\")\n",
    "        loss_weight = self.domain_weights.get(domain, 1.0)\n",
    "        \n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        \n",
    "        return x, y, torch.tensor(loss_weight)\n",
    "\n",
    "def main():\n",
    "    # Load configuration\n",
    "    with open(\"configs/model_remma.yaml\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = build_model(config[\"model\"]).to(device)  # Pass model config\n",
    "    \n",
    "    # Load combined dataset\n",
    "    try:\n",
    "        dataset = load_from_disk(\"data/processed/final_dataset\")\n",
    "    except FileNotFoundError:\n",
    "        raise RuntimeError(\"Dataset not found. Run data processing pipeline first.\")\n",
    "    \n",
    "    # Create dataloader with domain-aware sampling\n",
    "    train_dataset = MultiDomainDataset(dataset[\"train\"], config[\"model\"][\"block_size\"])\n",
    "    dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.get(\"batch_size\", 32),\n",
    "        shuffle=True,\n",
    "        num_workers=os.cpu_count() // 2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.get(\"learning_rate\", 3e-4),\n",
    "        weight_decay=config.get(\"weight_decay\", 0.1)\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config.get(\"total_steps\", 100_000),\n",
    "        eta_min=config.get(\"min_lr\", 1e-5)\n",
    "    )\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Training setup\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    grad_accum_steps = config.get(\"grad_accum_steps\", 1)  # Set default value if not in config\n",
    "    \n",
    "    # Initialize WandB\n",
    "    wandb_available = False\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb.init(project=\"remma-training\", config=config)\n",
    "        wandb_available = True\n",
    "        wandb.watch(model, log=\"all\")\n",
    "    except ImportError:\n",
    "        print(\"Wandb not installed, skipping logging\")\n",
    "    \n",
    "    # Progress tracking\n",
    "    progress_bar = tqdm(total=config.get(\"total_steps\", 100_000), desc=\"Training\")\n",
    "    \n",
    "    # Mixed-precision training loop\n",
    "    try:\n",
    "        while global_step < config.get(\"total_steps\", 100_000):\n",
    "            for x, y, weights in dataloader:\n",
    "                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "                weights = weights.to(device)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                with autocast(dtype=torch.bfloat16):\n",
    "                    logits, loss = model(x, targets=y)\n",
    "                    weighted_loss = (loss * weights).mean()\n",
    "                \n",
    "                scaler.scale(weighted_loss).backward()\n",
    "                if (global_step + 1) % grad_accum_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    scheduler.step()\n",
    "                \n",
    "                # Update tracking\n",
    "                global_step += 1\n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_postfix({\n",
    "                    \"loss\": weighted_loss.item(),\n",
    "                    \"lr\": optimizer.param_groups[0]['lr']\n",
    "                })\n",
    "                \n",
    "                # Log metrics\n",
    "                if wandb_available:\n",
    "                    wandb.log({\n",
    "                        \"loss\": weighted_loss.item(),\n",
    "                        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                        \"step\": global_step\n",
    "                    })\n",
    "                \n",
    "                # Checkpointing\n",
    "                if global_step % config.get(\"checkpoint_steps\", 10_000) == 0:\n",
    "                    save_checkpoint(\n",
    "                        model=model,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        step=global_step,\n",
    "                        config=config\n",
    "                    )\n",
    "                    \n",
    "                    # Validation (optional)\n",
    "                    if config.get(\"run_validation\", False):\n",
    "                        validation_loss = run_validation(model, device, config)\n",
    "                        if wandb_available:\n",
    "                            wandb.log({\"validation_loss\": validation_loss})\n",
    "                \n",
    "                if global_step >= config.get(\"total_steps\", 100_000):\n",
    "                    break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted. Saving final checkpoint...\")\n",
    "        save_checkpoint(model, optimizer, scheduler, global_step, config)\n",
    "    \n",
    "    finally:\n",
    "        progress_bar.close()\n",
    "        print(\"Training completed!\")\n",
    "\n",
    "def run_validation(model, device, config):\n",
    "    \"\"\"Optional validation loop\"\"\"\n",
    "    model.eval()\n",
    "    valid_dataset = load_from_disk(\"data/processed/validation_dataset\")\n",
    "    valid_loader = DataLoader(\n",
    "        MultiDomainDataset(valid_dataset, config[\"model\"][\"block_size\"]),\n",
    "        batch_size=config.get(\"val_batch_size\", 16),\n",
    "        num_workers=os.cpu_count() // 2\n",
    "    )\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with autocast(dtype=torch.bfloat16):\n",
    "                _, loss = model(x, targets=y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss / len(valid_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
