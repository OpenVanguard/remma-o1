model:
  # Added model architecture parameters
  name: "remma-o1-v1.0"
  vocab_size: 500017  # UPDATE THIS WITH ACTUAL VOCAB SIZE FROM TOKENIZER
  embed_size: 1024
  num_layers: 12
  num_heads: 8
  ff_dim: 4096
  block_size: 2048
  dropout: 0.1

training:
  dataset_path: "data/processed/final_dataset"
  # Remove domain_weights entirely
  total_steps: 500017  # Adjust as needed
  batch_size: 1  # Updated batch size

  learning_rate: 3e-4
  min_lr: 1e-5
  weight_decay: 0.1
  checkpoint_steps: 5000
