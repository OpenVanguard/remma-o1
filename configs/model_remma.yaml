model:
  # Added model architecture parameters
  name: "remma-o1-v1.0"
  vocab_size: 500017  # UPDATE THIS WITH ACTUAL VOCAB SIZE FROM TOKENIZER
  embed_size: 1024
  num_layers: 12
  num_heads: 8
  ff_dim: 4096
  block_size: 2048
  dropout: 0.1

training:
  dataset_path: "data/processed/wiki_only"
  # Consolidated training parameters
  total_steps: 500017
  batch_size: 32
  learning_rate: 3e-4
  min_lr: 1e-5
  weight_decay: 0.1
  warmup_steps: 2000
  math_weight: 1.5
  checkpoint_steps: 10000
  val_batch_size: 16
  run_validation: true
  domain_weights:
    wikipedia: 0.8
    math: 0.2